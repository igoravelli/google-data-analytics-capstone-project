# Process to clean all dataframes

### Dataframe schema

| Column | Description | DType |
| :--- | :--- | :--- |
| `realSum` | the total price of the listing | integer |
| `room_type` | private/shared/entire home/apt | string |
| `room_shared` | indicate if the room is shared | bool |
| `room_private` | indicate if the room is private | bool |
| `person_capacity` | number of people capacity in the room | integer |
| `host_is_superhost` | boolean value indicating if host is a superhost or not | bool |
| `multi` | indicator whether listing is for multiple rooms or not | integer |
| `biz` | business indicator | integer |
| `cleanliness_rating` | rating of the clean level of the room | integer |
| `guest_satisfaction_overall` | overall rating from guests camparing all listings offered by host | integer |
| `bedrooms` | quantity of bedrooms available in the listing | integer |
| `dist` | distance from city center | float |
| `metro_dist` | distance from closest subway station | float |
| `lng & lat` | coordinates for location identification | float |

# Install & load packages
```{r install & load packages}
# install.packages("dplyr")
library(dplyr)
library(plyr)
library(knitr)
```


# Loading all files
In the code below we read all CSV files in the folder "csv_files" and create the "city" column that indicates the city of the listing, and the "type" column that
indicate if the listing is about weekdays or weekends.

```{r load all raw files}
setwd("/home/igorregly/google-data-analytics-capstone-project/csv_files/")
dfs <- list()
listcsv <- dir(pattern = "*.csv")

for (k in seq_along(listcsv)) {
    city_n_type <- strsplit(listcsv[k], ".csv")[[1]]
    city <- strsplit(city_n_type, "_")[[1]][1]
    type <- strsplit(city_n_type, "_")[[1]][2]

    dfs[[k]] <- read.csv(listcsv[k])

    dfs[[k]] <- cbind(dfs[[k]], city, type)
}
```

After all CSV files had been read, we merged them all into a unique data frame to make it easier to analyze the data.
```{r make the unique dataframe}
unique_df <- ldply(dfs, data.frame)
```

---
# Cleaning the data
## Filtering out null or empty rows
In the chunk below we remove all rows with null values or rows that are empty because these rows will mess up our analysis.

```{r}
all_data <-
    unique_df[
        Reduce(`&`, lapply(unique_df, function(x) !(is.na(x) | x == ""))),
    ]

print(nrow(unique_df))
print(nrow(all_data))
```

## Checking duplicates
Now we'll check on the data if there are any duplicated rows and, if so, remove them.

```{r}
all_data <- unique(all_data)
View(all_data)
```

Below we check the summary of the data
```{r check summary of the data}
View(summary(all_data))
```

## Create aggregated csv file with all data

Now we'll create the aggregated csv file with all data about all countries and its specs
```{r generate aggregated csv file}
write.csv(all_data, "~/google-data-analytics-capstone-project/aggregated_data.csv", row.names=FALSE)
```